{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6add8cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seolho/anaconda3/envs/urlb/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/seolho/anaconda3/envs/urlb/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "/home/seolho/anaconda3/envs/urlb/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n"
     ]
    }
   ],
   "source": [
    "from finetune import Workspace as fW\n",
    "from pretrain import Workspace as pW\n",
    "import os\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d32b282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import hydra\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "config_dir = pathlib.Path('./')\n",
    "hydra.initialize(config_path=config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49954b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace: /home/seolho/workplace/self-supervised-rl/for commit/cic\n"
     ]
    }
   ],
   "source": [
    "cfg = hydra.compose(config_name='finetune.yaml', overrides=[])\n",
    "f_workspace = fW(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4b078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[32meval\u001b[0m  | F: 0 | S: 0 | E: 0 | L: 1000 | R: 193.4848 | T: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "skill = np.zeros(64).astype(np.float32)\n",
    "f_workspace.eval(skill)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62c84f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace: /home/seolho/workplace/self-supervised-rl/for commit/cic\n"
     ]
    }
   ],
   "source": [
    "cfg = hydra.compose(config_name='pretrain.yaml', overrides=[])\n",
    "p_workspace = pW(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affc7dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': {'_target_': 'agent.cic.CICAgent', 'name': 'cic', 'reward_free': '${reward_free}', 'obs_type': 'states', 'obs_shape': [24], 'action_shape': [6], 'device': '${device}', 'lr': 0.0001, 'critic_target_tau': 0.01, 'update_every_steps': 2, 'use_tb': '${use_tb}', 'use_wandb': '${use_wandb}', 'num_expl_steps': 4000, 'hidden_dim': 1024, 'feature_dim': 1024, 'stddev_schedule': 0.2, 'stddev_clip': 0.3, 'skill_dim': 64, 'scale': 1.0, 'update_skill_every_step': 50, 'nstep': 3, 'batch_size': 128, 'project_skill': True, 'init_critic': True, 'rew_type': 'og', 'update_rep': True, 'temp': 0.5}, 'reward_free': True, 'domain': 'walker', 'obs_type': 'states', 'frame_stack': 3, 'action_repeat': 1, 'discount': 0.99, 'num_train_frames': 2000010, 'num_seed_frames': 4000, 'eval_every_frames': 10000, 'num_eval_episodes': 10, 'snapshots': [100000, 500000, 1000000, 2000000], 'snapshot_dir': '../../../pretrained_models/${obs_type}/${domain}/${agent.name}/${experiment}', 'replay_buffer_size': 1000000, 'replay_buffer_num_workers': 4, 'batch_size': '${agent.batch_size}', 'nstep': '${agent.nstep}', 'update_encoder': True, 'seed': 1, 'device': 'cuda', 'save_video': True, 'save_train_video': False, 'use_tb': True, 'use_wandb': False, 'experiment': 'exp'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba4bd4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12050"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.random.uniform(0,1,64).astype(np.float32)\n",
    "skill = np.zeros(64).astype(np.float32)\n",
    "#skill = np.random.uniform(0,1,64).astype(np.float32)\n",
    "#workspace.eval(skill)\n",
    "\n",
    "f_workspace.gather_trajectories(skill, 10000)\n",
    "len(f_workspace.replay_storage)\n",
    "\n",
    "#p_workspace.agent.behavior_cloning(f_workspace.replay_iter, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "517a74f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[32meval\u001b[0m  | F: 0 | S: 0 | E: 0 | L: 1000 | R: 19.3694 | T: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "skill = np.zeros(64).astype(np.float32)\n",
    "p_workspace.eval(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98bde1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_reward': 0.1391632854938507, 'bc_loss': 0.4016333818435669}\n",
      "{'batch_reward': 0.14747251570224762, 'bc_loss': 0.3905754089355469}\n",
      "{'batch_reward': 0.14643533527851105, 'bc_loss': 0.38684481382369995}\n",
      "{'batch_reward': 0.16485702991485596, 'bc_loss': 0.3821464478969574}\n",
      "{'batch_reward': 0.14301274716854095, 'bc_loss': 0.3958275020122528}\n",
      "{'batch_reward': 0.13627636432647705, 'bc_loss': 0.3756754398345947}\n",
      "{'batch_reward': 0.14457392692565918, 'bc_loss': 0.37170514464378357}\n",
      "{'batch_reward': 0.1434091031551361, 'bc_loss': 0.40581852197647095}\n",
      "{'batch_reward': 0.14798255264759064, 'bc_loss': 0.38474246859550476}\n",
      "{'batch_reward': 0.14991886913776398, 'bc_loss': 0.39812788367271423}\n",
      "{'batch_reward': 0.14860355854034424, 'bc_loss': 0.3977634906768799}\n",
      "{'batch_reward': 0.14146144688129425, 'bc_loss': 0.3843810558319092}\n",
      "{'batch_reward': 0.14360158145427704, 'bc_loss': 0.39050421118736267}\n",
      "{'batch_reward': 0.14936241507530212, 'bc_loss': 0.39078444242477417}\n",
      "{'batch_reward': 0.16509610414505005, 'bc_loss': 0.38149333000183105}\n",
      "{'batch_reward': 0.12736016511917114, 'bc_loss': 0.37830036878585815}\n",
      "{'batch_reward': 0.1559469997882843, 'bc_loss': 0.37262770533561707}\n",
      "{'batch_reward': 0.14616426825523376, 'bc_loss': 0.3601020276546478}\n",
      "{'batch_reward': 0.1469283401966095, 'bc_loss': 0.37892597913742065}\n",
      "{'batch_reward': 0.1441938877105713, 'bc_loss': 0.38364529609680176}\n",
      "{'batch_reward': 0.1303422451019287, 'bc_loss': 0.39063379168510437}\n",
      "{'batch_reward': 0.15731149911880493, 'bc_loss': 0.4052485227584839}\n",
      "{'batch_reward': 0.14584816992282867, 'bc_loss': 0.3965708911418915}\n",
      "{'batch_reward': 0.13659527897834778, 'bc_loss': 0.3893427848815918}\n",
      "{'batch_reward': 0.13942712545394897, 'bc_loss': 0.3949693441390991}\n",
      "{'batch_reward': 0.15829740464687347, 'bc_loss': 0.3786385953426361}\n",
      "{'batch_reward': 0.1381722390651703, 'bc_loss': 0.3938552737236023}\n",
      "{'batch_reward': 0.16945350170135498, 'bc_loss': 0.38405200839042664}\n",
      "{'batch_reward': 0.15430143475532532, 'bc_loss': 0.3726966381072998}\n",
      "{'batch_reward': 0.1222275048494339, 'bc_loss': 0.3752206265926361}\n",
      "{'batch_reward': 0.15742355585098267, 'bc_loss': 0.39210066199302673}\n",
      "{'batch_reward': 0.1463925838470459, 'bc_loss': 0.39418095350265503}\n",
      "{'batch_reward': 0.1485409438610077, 'bc_loss': 0.3816158175468445}\n",
      "{'batch_reward': 0.14951986074447632, 'bc_loss': 0.3924688696861267}\n",
      "{'batch_reward': 0.13605402410030365, 'bc_loss': 0.3773910403251648}\n",
      "{'batch_reward': 0.14882123470306396, 'bc_loss': 0.36384743452072144}\n",
      "{'batch_reward': 0.1528189480304718, 'bc_loss': 0.3956013321876526}\n",
      "{'batch_reward': 0.14793601632118225, 'bc_loss': 0.39424633979797363}\n",
      "{'batch_reward': 0.14589497447013855, 'bc_loss': 0.3898025453090668}\n",
      "{'batch_reward': 0.15020668506622314, 'bc_loss': 0.3985428214073181}\n",
      "{'batch_reward': 0.14614132046699524, 'bc_loss': 0.40134257078170776}\n",
      "{'batch_reward': 0.16781030595302582, 'bc_loss': 0.3705729842185974}\n",
      "{'batch_reward': 0.14703607559204102, 'bc_loss': 0.38462480902671814}\n",
      "{'batch_reward': 0.15330807864665985, 'bc_loss': 0.3763279914855957}\n",
      "{'batch_reward': 0.16788645088672638, 'bc_loss': 0.37872570753097534}\n",
      "{'batch_reward': 0.1701073795557022, 'bc_loss': 0.3910423219203949}\n",
      "{'batch_reward': 0.14060620963573456, 'bc_loss': 0.39503777027130127}\n",
      "{'batch_reward': 0.1600838154554367, 'bc_loss': 0.37665170431137085}\n",
      "{'batch_reward': 0.16233503818511963, 'bc_loss': 0.3936874270439148}\n",
      "{'batch_reward': 0.15532344579696655, 'bc_loss': 0.39528152346611023}\n",
      "{'batch_reward': 0.13276460766792297, 'bc_loss': 0.37881749868392944}\n",
      "{'batch_reward': 0.13513672351837158, 'bc_loss': 0.36231231689453125}\n",
      "{'batch_reward': 0.13017985224723816, 'bc_loss': 0.36359700560569763}\n",
      "{'batch_reward': 0.15318456292152405, 'bc_loss': 0.3845004737377167}\n",
      "{'batch_reward': 0.1360556036233902, 'bc_loss': 0.3705398738384247}\n",
      "{'batch_reward': 0.17078961431980133, 'bc_loss': 0.3946468234062195}\n",
      "{'batch_reward': 0.13868758082389832, 'bc_loss': 0.3945147395133972}\n",
      "{'batch_reward': 0.12749996781349182, 'bc_loss': 0.37795743346214294}\n",
      "{'batch_reward': 0.14344540238380432, 'bc_loss': 0.3774387538433075}\n",
      "{'batch_reward': 0.16248193383216858, 'bc_loss': 0.34895190596580505}\n",
      "{'batch_reward': 0.12963134050369263, 'bc_loss': 0.3988656997680664}\n",
      "{'batch_reward': 0.13416637480258942, 'bc_loss': 0.35105448961257935}\n",
      "{'batch_reward': 0.14381951093673706, 'bc_loss': 0.39129549264907837}\n",
      "{'batch_reward': 0.1486271619796753, 'bc_loss': 0.36179614067077637}\n",
      "{'batch_reward': 0.14832322299480438, 'bc_loss': 0.38522064685821533}\n",
      "{'batch_reward': 0.13512372970581055, 'bc_loss': 0.3811894655227661}\n",
      "{'batch_reward': 0.150390625, 'bc_loss': 0.37355393171310425}\n",
      "{'batch_reward': 0.16564248502254486, 'bc_loss': 0.3856646418571472}\n",
      "{'batch_reward': 0.15861406922340393, 'bc_loss': 0.3515792787075043}\n",
      "{'batch_reward': 0.14023229479789734, 'bc_loss': 0.3741189241409302}\n",
      "{'batch_reward': 0.14106608927249908, 'bc_loss': 0.368874728679657}\n",
      "{'batch_reward': 0.13659578561782837, 'bc_loss': 0.3881291151046753}\n",
      "{'batch_reward': 0.14625334739685059, 'bc_loss': 0.3707699775695801}\n",
      "{'batch_reward': 0.16027751564979553, 'bc_loss': 0.3799861669540405}\n",
      "{'batch_reward': 0.14771398901939392, 'bc_loss': 0.39269790053367615}\n",
      "{'batch_reward': 0.1429213285446167, 'bc_loss': 0.3792109191417694}\n",
      "{'batch_reward': 0.137828528881073, 'bc_loss': 0.40333089232444763}\n",
      "{'batch_reward': 0.15038138628005981, 'bc_loss': 0.3775906264781952}\n",
      "{'batch_reward': 0.14384111762046814, 'bc_loss': 0.3745441436767578}\n",
      "{'batch_reward': 0.15474288165569305, 'bc_loss': 0.3878905177116394}\n",
      "{'batch_reward': 0.16189315915107727, 'bc_loss': 0.36731433868408203}\n",
      "{'batch_reward': 0.14851966500282288, 'bc_loss': 0.3618045449256897}\n",
      "{'batch_reward': 0.14390164613723755, 'bc_loss': 0.3799540400505066}\n",
      "{'batch_reward': 0.14091119170188904, 'bc_loss': 0.35274749994277954}\n",
      "{'batch_reward': 0.1529756486415863, 'bc_loss': 0.39798206090927124}\n",
      "{'batch_reward': 0.13711731135845184, 'bc_loss': 0.38840025663375854}\n",
      "{'batch_reward': 0.14807526767253876, 'bc_loss': 0.3666565418243408}\n",
      "{'batch_reward': 0.16525349020957947, 'bc_loss': 0.37654393911361694}\n",
      "{'batch_reward': 0.16552719473838806, 'bc_loss': 0.3800017833709717}\n",
      "{'batch_reward': 0.1418096423149109, 'bc_loss': 0.4002285897731781}\n",
      "{'batch_reward': 0.15504090487957, 'bc_loss': 0.3660101294517517}\n",
      "{'batch_reward': 0.16188496351242065, 'bc_loss': 0.3931802213191986}\n",
      "{'batch_reward': 0.15293800830841064, 'bc_loss': 0.38579750061035156}\n",
      "{'batch_reward': 0.14361947774887085, 'bc_loss': 0.3893086910247803}\n",
      "{'batch_reward': 0.14575403928756714, 'bc_loss': 0.3565517067909241}\n",
      "{'batch_reward': 0.12798726558685303, 'bc_loss': 0.39599451422691345}\n",
      "{'batch_reward': 0.1431821584701538, 'bc_loss': 0.3859562873840332}\n",
      "{'batch_reward': 0.1595524549484253, 'bc_loss': 0.38974878191947937}\n",
      "{'batch_reward': 0.13771936297416687, 'bc_loss': 0.3722490966320038}\n",
      "{'batch_reward': 0.14431074261665344, 'bc_loss': 0.3878346383571625}\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    #skill = np.random.uniform(0,1,64).astype(np.float32)\n",
    "    skill = np.zeros(64).astype(np.float32)\n",
    "    #workspace.eval(skill)\n",
    "    \n",
    "    f_workspace.gather_trajectories(skill, 1000)\n",
    "    len(f_workspace.replay_storage)\n",
    "\n",
    "    print(p_workspace.agent.behavior_cloning(f_workspace.replay_iter, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b25a799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[32meval\u001b[0m  | F: 0 | S: 0 | E: 0 | L: 1000 | R: 21.4710 | T: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "skill = np.zeros(64).astype(np.float32)\n",
    "p_workspace.eval(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a127ea6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a15c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b3250a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Until:\n",
    "    def __init__(self, until, action_repeat=1):\n",
    "        self._until = until\n",
    "        self._action_repeat = action_repeat\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self._until is None:\n",
    "            return True\n",
    "        until = self._until // self._action_repeat\n",
    "        return step < until"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff4ef658",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Until(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def behavior_cloning(self, replay_iter, step):\n",
    "        metrics = dict()\n",
    "\n",
    "        if step % self.update_every_steps != 0:\n",
    "            return metrics\n",
    "\n",
    "        batch = next(replay_iter)\n",
    "\n",
    "        obs, action, reward, discount, next_obs, skill = utils.to_torch(\n",
    "            batch, self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            obs = self.aug_and_encode(obs)\n",
    "        \n",
    "            next_obs = self.aug_and_encode(next_obs)\n",
    "\n",
    "\n",
    "        if self.use_tb or self.use_wandb:\n",
    "            metrics['batch_reward'] = reward.mean().item()\n",
    "\n",
    "        # extend observations with skill\n",
    "        obs = torch.cat([obs, skill], dim=1)\n",
    "        next_obs = torch.cat([next_obs, skill], dim=1)\n",
    "        \n",
    "        stddev = utils.schedule(self.stddev_schedule, step)\n",
    "        dist = self.actor(obs, stddev)\n",
    "        agent_action = dist.sample(clip=self.stddev_clip)\n",
    "        \n",
    "        bc_loss = torch.mean((action - agent_action)**2)\n",
    "        \n",
    "        self.actor_opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        self.actor_opt.step()\n",
    "        # update actor\n",
    "        metrics['bc_loss'] = bc_loss.item()\n",
    "\n",
    "        return metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urlb",
   "language": "python",
   "name": "urlb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
